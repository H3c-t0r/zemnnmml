---
description: Learn how to track RAG-associated artifacts with ZenML.
---

## Tracking RAG-Associated Artifacts with ZenML

You can see in the code above how we've manually logged the metadata for the
artifacts that are generated by the pipeline. This is a good practice to follow
because it allows you to track the evolution of the pipeline over time and
understand how changes to the pipeline affect the performance of the RAG models.

ZenML allows you to track all the artifacts associated with your RAG pipeline,
from hyperparameters and model weights to metadata and performance metrics, as
well as all the RAG or LLM-specific artifacts like chains, agents, tokenizers
and vector stores. These can all be tracked in the [Model Control
Plane](../advanced-guide/data-management/model-management.md) and thus
visualized in the [ZenML Cloud](https://zenml.io/cloud) dashboard.

By bringing all of the above into a simple ZenML
pipeline we achieve a clearly delineated set of steps that can be run and rerun to set up
our basic RAG pipeline. This is a great starting point for building out more
complex RAG pipelines, and it's a great way to get started with LLMs in a
sensible way.

Some of the advantages that ZenML brings to the table here include:

- **Reproducibility**: You can rerun the pipeline to update the index store with
  new documents or to change the parameters of the chunking process and so on. Previous versions of
  the artifacts will be preserved, and you can compare the performance of
    different versions of the pipeline.
- **Scalability**: You can easily scale the pipeline to handle larger corpora of
    documents by deploying it on a cloud provider and using a more scalable
    vector store.
- **Tracking artifacts and associating them with metadata**: You can track the
    artifacts generated by the pipeline and associate them with metadata that
    provides additional context and insights into the pipeline. This metadata
    and these artifacts are then visible in the ZenML dashboard, allowing you to
    monitor the performance of the pipeline and debug any issues that arise.
- **Maintainability** - Having your pipeline in a clear, modular format makes it
    easier to maintain and update. You can easily add new steps, change the
    parameters of existing steps, and experiment with different configurations
    to see how they affect the performance of the pipeline.
- **Collaboration** - You can share the pipeline with your team and collaborate
    on it together. You can also use the ZenML dashboard to share insights and
    findings with your team, making it easier to work together on the pipeline.

In the next section, we'll showcase a basic RAG inference pipeline that uses the
embeddings generated by the pipeline to retrieve the most relevant chunks of
text based on a given query. This will give you a taste of how you can leverage
the power of LLMs in your MLOps workflows using ZenML.

<!-- For scarf -->
<figure><img alt="ZenML Scarf" referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc" /></figure>
