---
description: Track how your RAG pipeline improves using evaluation and metrics.
---

We've seen retrieval and generation pieces now.

# Assessing performance

- our specific data / examples with charts etc
- caveats
LLM judge is expensive
takes time to run
automating doesn't replace the focus on the details / data

- ideal world: how / how much and when are you evaluating

# Next steps:

adding a reranker to improve our retrieval

<figure><img src="https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc" alt="ZenML Scarf"><figcaption></figcaption></figure>
