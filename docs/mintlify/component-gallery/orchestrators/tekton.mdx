---
title: "Tekton Orchestrator"
description: "How to orchestrate pipelines with Tekton"
---

The Tekton orchestrator is an [orchestrator](/component-gallery/orchestrators) flavor provided with the ZenML `tekton` integration that uses [Tekton Pipelines](https://tekton.dev/) to run your pipelines.

This component is only meant to be used within the context of [remote ZenML deployment scenario](/getting-started/deploying-zenml). Usage with a local ZenML deployment may lead to unexpected behavior!

### When to use it

You should use the Tekton orchestrator if:

* you're looking for a proven production-grade orchestrator.

* you're looking for a UI in which you can track your pipeline runs.

* you're already using Kubernetes or are not afraid of setting up and maintaining a Kubernetes cluster.

* you're willing to deploy and maintain Tekton Pipelines on your cluster.

### How to deploy it

You'll first need to set up a Kubernetes cluster and deploy Tekton Pipelines:

AWS

GCP

Azure

* A remote ZenML server. See the [deployment guide](/getting-started/deploying-zenml) for more information.

* Have an existing AWS [EKS cluster](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html) set up.

* Make sure you have the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) set up.

* Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and [configure](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-connection/) it to talk to your EKS cluster using the following command:  
```  
aws eks --region REGION update-kubeconfig --name CLUSTER_NAME  
```

* [Install](https://tekton.dev/docs/pipelines/install/) Tekton Pipelines onto your cluster.

* A remote ZenML server. See the [deployment guide](/getting-started/deploying-zenml) for more information.

* Have an existing GCP [GKE cluster](https://cloud.google.com/kubernetes-engine/docs/quickstart) set up.

* Make sure you have the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk) set up first.

* Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and [configure](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl) it to talk to your GKE cluster using the following command:  
```  
gcloud container clusters get-credentials CLUSTER_NAME  
```

* [Install](https://tekton.dev/docs/pipelines/install/) Tekton Pipelines onto your cluster.

* A remote ZenML server. See the [deployment guide](/getting-started/deploying-zenml) for more information.

* Have an existing [AKS cluster](https://azure.microsoft.com/en-in/services/kubernetes-service/#documentation) set up.

* Make sure you have the [az CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) set up first.

* Download and [install](https://kubernetes.io/docs/tasks/tools/) `kubectl` and it to talk to your AKS cluster using the following command:  
```  
az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER_NAME  
```

* [Install](https://tekton.dev/docs/pipelines/install/) Tekton Pipelines onto your cluster.

If one or more of the deployments are not in the `Running` state, try increasing the number of nodes in your cluster.

ZenML has only been tested with Tekton Pipelines >=0.38.3 and may not work with previous versions.

### How to use it

To use the Tekton orchestrator, we need:

* The ZenML `tekton` integration installed. If you haven't done so, run  
```  
zenml integration install tekton -y  
```

* [Docker](https://www.docker.com) installed and running.

* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) installed.

* Tekton pipelines deployed on a remote cluster. See the [deployment section](/component-gallery/orchestrators/tekton#how-to-deploy-it) for more information.

* The name of your Kubernetes context which points to your remote cluster. Run `kubectl config get-contexts` to see a list of available contexts.

* A [remote artifact store](/component-gallery/artifact-stores) as part of your stack.

* A [remote container registry](/component-gallery/container-registries) as part of your stack.

We can then register the orchestrator and use it in our active stack:

```
zenml orchestrator register  \
    --flavor=tekton \
    --kubernetes_context=

# Register and activate a stack with the new orchestrator
zenml stack register  -o  ... --set
```

ZenML will build a Docker image called `<CONTAINER_REGISTRY_URI>/zenml:<PIPELINE_NAME>` which includes your code and use it to run your pipeline steps in Tekton. Check out [this page](/advanced-guide/pipelines/containerization) if you want to learn more about how ZenML builds these images and how you can customize them.

Once the orchestrator is part of the active stack, we need to run `zenml stack up` before running any pipelines. This command forwards a port, so you can view the Tekton UI in your browser.

You can now run any ZenML pipeline using the Tekton orchestrator:

```
python file_that_runs_a_zenml_pipeline.py
```

#### Additional configuration

For additional configuration of the Tekton orchestrator, you can pass `TektonOrchestratorSettings` which allows you to configure (among others) the following attributes:

* `pod_settings`: Node selectors, affinity and tolerations to apply to the Kubernetes Pods running your pipline. These can be either specified using the Kubernetes model objects or as dictionaries.

```
from zenml.integrations.tekton.flavors.tekton_orchestrator_flavor import TektonOrchestratorSettings
from kubernetes.client.models import V1Toleration

tekton_settings = TektonOrchestratorSettings(
    pod_settings={
        "affinity": {
            "nodeAffinity": {
                "requiredDuringSchedulingIgnoredDuringExecution": {
                    "nodeSelectorTerms": [
                        {
                            "matchExpressions": [
                                {
                                    "key": "node.kubernetes.io/name",
                                    "operator": "In",
                                    "values": ["my_powerful_node_group"],
                                }
                            ]
                        }
                    ]
                }
            }
        },
        "tolerations": [
            V1Toleration(
                key="node.kubernetes.io/name",
                operator="Equal",
                value="",
                effect="NoSchedule"
            )
        ]
    }
)

@pipeline(
    settings={
        "orchestrator.tekton": tekton_settings
    }
)
  ...
```

Check out the [API docs](https://apidocs.zenml.io/latest/integration%5Fcode%5Fdocs/integrations-tekton/#zenml.integrations.tekton.flavors.tekton%5Forchestrator%5Fflavor.TektonOrchestratorSettings) for a full list of available attributes and [this docs page](/advanced-guide/pipelines/settings) for more information on how to specify settings.

A concrete example of using the Tekton orchestrator can be found [here](https://github.com/zenml-io/zenml/tree/main/examples/tekton%5Fpipelines%5Forchestration).

For more information and a full list of configurable attributes of the Tekton orchestrator, check out the [API Docs](https://apidocs.zenml.io/latest/integration%5Fcode%5Fdocs/integrations-tekton/#zenml.integrations.tekton.orchestrators.tekton%5Forchestrator.TektonOrchestrator).

#### Enabling CUDA for GPU-backed hardware

Note that if you wish to use this orchestrator to run steps on a GPU, you will need to follow [the instructions on this page](/advanced-guide/pipelines/gpu-hardware) to ensure that it works. It requires adding some extra settings customization and is essential to enable CUDA for the GPU to give its full acceleration.